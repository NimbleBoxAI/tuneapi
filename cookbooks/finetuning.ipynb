{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Finetuning on Tune Studio using `tuneapi`\n",
                "\n",
                "In this example we will create a list of threads (chats) that we want to use as our training sample. Then upload it to Tune Studio and use it to train a `meta/llama-3-8b-instruct-8k` model.\n",
                "\n",
                "All using `tuneapi` package so you can use it in your code too."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from tuneapi import types as tt\n",
                "from tuneapi import endpoints as te\n",
                "\n",
                "# ðŸ¤— datasets\n",
                "from datasets import load_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'id': 'JbWB9D',\n",
                            " 'conversations': [{'from': 'system',\n",
                            "   'value': 'You are \"Ranker Assistant\" whose job is to rated each response on a Likert 5 scale (between 0 and 4) for each of these attributes:\\n\\n1.  Helpfulness: Overall helpfulness of the response to the prompt.\\n2.  Correctness: Inclusion of all pertinent facts without errors.\\n3.  Coherence: Consistency and clarity of expression.\\n4.  Complexity: Intellectual depth required to write response (i.e. whether the response can be written by anyone with basic\\n    language competency or requires deep domain expertise).\\n5.  Verbosity: Amount of detail included in the response, relative to what is asked for in the prompt.\\n\\n\\nSome prompt can be multi-turn. In this case, the prompt consists of all of the user turns and all but the last assistant\\nturn, which is contained in the response field. This is done because the attribute values only assessed only for the last\\nassistant turn. For multi-turn prompts, the structure of prompts look like below.\\n\\n```\\n<start>\\n{user turn 1}\\n<extra_id_1>Assistant\\n{assistant turn 1}\\n<extra_id_1>User\\n{user turn 2}\\n<final>\\n{response}\\n```\\n\\nYou will always respond with the template format:\\n\\n```\\n- helpfulness: <number>\\n- correctness: <number>\\n- coherence: <number>\\n- complexity: <number>\\n- verbosity: <number>\\n```\\n\\nRemember these values should be between [0, 4] with:\\n- 0 being the worst score\\n- 4 being the highest score'},\n",
                            "  {'from': 'human',\n",
                            "   'value': '<start>\\nc#\\n<final>C# is a high-level, object-oriented programming language developed by Microsoft as part of its .NET initiative. It was created as a modern alternative to Java and supports a variety of programming paradigms, including imperative, functional, and event-driven. C# is primarily used for Windows application development, but it can also be used for web, mobile, and game development. The language is designed to be safe, secure, and efficient, and it provides developers with a rich set of libraries and tools for building robust and scalable applications. C# is also widely used in the game development industry, particularly in the development of games for the Xbox 360 and Xbox One consoles.'},\n",
                            "  {'from': 'gpt',\n",
                            "   'value': '- helpfulness: 3\\n- correctness: 4\\n- coherence: 4\\n- complexity: 2\\n- verbosity: 1'}]}"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# this is a sample pseudo-ranker dataset I built from `nvidia/HelpSteer2` to fit Tune platform\n",
                "ds = load_dataset(\"yashnbx/hs2-tune\")[\"train\"]\n",
                "print(ds[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "# build a threads list from `yashnbx/hs2-tune` dataset\n",
                "if not os.path.exists(\"./hs2-cookbook\"):\n",
                "    ds = load_dataset(\"yashnbx/hs2-tune\")[\"train\"]\n",
                "    threads = tt.ThreadsList()\n",
                "    for i in range(200):\n",
                "        threads.append(tt.Thread.from_dict(ds[i]))\n",
                "    print(threads)\n",
                "    threads.to_disk(\"./hs2-cookbook\")\n",
                "else:\n",
                "    # already save, just deserialize\n",
                "    threads = tt.ThreadsList.from_disk(\"./hs2-cookbook\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[2024-08-11T16:27:54+0530] [WARNING] [chats.py:1024] exporting to ft format, you cannot recreate the dataset from this.\n",
                        "[2024-08-11T16:27:54+0530] [INFO] [finetune.py:66] Dataset saved to tuneds/hs2-test/tuneds.jsonl\n",
                        "[2024-08-11T16:27:55+0530] [INFO] [finetune.py:92] Upload successful!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "FTDataset(path='datasets/chat/hs2-test.jsonl', type='relic')\n"
                    ]
                }
            ],
            "source": [
                "ft = te.FinetuningAPI(\n",
                "    tune_api_key=\"xxxxx\",\n",
                "    tune_org_id=\"yyyyy\", # ignore this if you don't have multiple orgs\n",
                ")\n",
                "out = ft.upload_dataset(\n",
                "    threads=threads,\n",
                "    name=\"hs2-test\",\n",
                "    override=True\n",
                ")\n",
                "print(out)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[2024-08-11T15:51:29+0530] [INFO] [finetune.py:131] Finetuning job created with ID: azxrbqsj. Check progress at: https://studio.tune.app/finetuning/azxrbqsj?org_id=e3365ae7-ceeb-425b-b983-1703e8456f76\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "<TuneModel: ranker2-demo-model-azxrbqsj | e3365ae7-ceeb-425b-b983-1703e8456f76>"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# You can finetune on multiple datasets at once so pass them as a list, this function will return a model object\n",
                "# once the model training is complete, this will take some time\n",
                "model = ft.finetune(\"ranker2-demo\", datasets=[out])\n",
                "model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'id': 'azxrbqsj',\n",
                            " 'name': 'ranker2-demo',\n",
                            " 'resource': {'gpu': 'nvidia-l4',\n",
                            "  'gpuCount': '1',\n",
                            "  'diskSize': '30Gi',\n",
                            "  'maxRetries': 1},\n",
                            " 'meta': {'metadata': {'base_model_id': '5fmycsn2',\n",
                            "   'modality': 1,\n",
                            "   'training_config': {'adapter': 'qlora',\n",
                            "    'base_model': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
                            "    'base_model_config': 'meta-llama/Meta-Llama-3-8B-Instruct',\n",
                            "    'chat_template': 'llama3',\n",
                            "    'datasets': [{'conversation': 'llama3',\n",
                            "      'data_files': '/root/.cache/model/chat/hs2-test.jsonl',\n",
                            "      'ds_type': 'json',\n",
                            "      'path': '/root/.cache/model/chat/hs2-test.jsonl',\n",
                            "      'type': 'sharegpt'}],\n",
                            "    'eval_sample_packing': False,\n",
                            "    'eval_steps': 50,\n",
                            "    'flash_attention': True,\n",
                            "    'gradient_accumulation_steps': 4,\n",
                            "    'gradient_checkpointing': True,\n",
                            "    'hf_use_auth_token': True,\n",
                            "    'learning_rate': 0.0001,\n",
                            "    'load_in_4bit': True,\n",
                            "    'logging_steps': 1,\n",
                            "    'lora_alpha': 16,\n",
                            "    'lora_dropout': 0.05,\n",
                            "    'lora_r': 32,\n",
                            "    'lora_target_linear': True,\n",
                            "    'lr_scheduler': 'cosine',\n",
                            "    'micro_batch_size': 2,\n",
                            "    'model_type': 'AutoModelForCausalLM',\n",
                            "    'num_epochs': 3,\n",
                            "    'optimizer': 'paged_adamw_32bit',\n",
                            "    'output_dir': '/root/.cache/model/yash/ranker2-demo-model-azxrbqsj',\n",
                            "    'pad_to_sequence_len': True,\n",
                            "    'sample_packing': True,\n",
                            "    'save_safetensors': True,\n",
                            "    'sequence_len': 4096,\n",
                            "    'special_tokens': {'pad_token': '<|end_of_text|>'},\n",
                            "    'tokenizer_type': 'AutoTokenizer',\n",
                            "    'warmup_steps': 10},\n",
                            "   'weight_location': ''}},\n",
                            " 'status': 'PROVISIONING',\n",
                            " 'createdAt': '2024-08-11T10:21:28.861Z',\n",
                            " 'updatedAt': '2024-08-11T10:21:29.073Z'}"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# to get the job status you can keep calling this function\n",
                "ft.get_job(\"azxrbqsj\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
